{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator is a library I've created (https://github.com/LLCampos/evaluator)\n",
    "from Evaluator import Evaluator\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Normalized format is \"{index_begin}:{index_end}:{entity_name}\". For example:\n",
    "# \"191:200:carcinoma\"\n",
    "\n",
    "# Convert MER annotation into a normalized format\n",
    "# For example, converts \"191\\t200\\tcarcinoma\\n\" into \"191:200:carcinoma\"\n",
    "def normalize_tsv_mer_annotations(mer_annotation):\n",
    "    return mer_annotation.strip().replace('\\t', ':')\n",
    "\n",
    "# Convert Standoff annotation into a normalized format\n",
    "# For example, converts \"[40::58]\\tHP_0000006 | autosomal dominant\\n\" into \"40:58:autosomal dominant\"\n",
    "def normalize_standoff_annotation(standoff_annotation):\n",
    "    index_begin = re.findall('\\[(.*)::', standoff_annotation)[0]\n",
    "    index_end = re.findall('::(.*)]', standoff_annotation)[0]\n",
    "    entity_name = re.findall('\\| (.*)', standoff_annotation)[0]\n",
    "    \n",
    "    return '{}:{}:{}'.format(index_begin, index_end, entity_name)\n",
    "    \n",
    "\n",
    "def macro_evaluation(path_to_gold_annotations, path_to_test_annotations):\n",
    "    \n",
    "    document_ids = os.listdir(path_to_test_annotations)\n",
    "    # Remove extension\n",
    "    document_ids = map(lambda document_id: document_id.split('.')[0], document_ids)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    fscores = []\n",
    "    number_documents = len(document_ids)\n",
    "    \n",
    "    for document_id in document_ids:\n",
    "        with open(path_to_test_annotations + document_id + '.tsv') as f:\n",
    "            mer_annotations_tsv = f.readlines()\n",
    "\n",
    "        with open(path_to_gold_annotations + document_id) as f:\n",
    "            gold_annotations_standoff = f.readlines()\n",
    "\n",
    "        mer_annotations = map(lambda annotation: normalize_tsv_mer_annotations(annotation), mer_annotations_tsv) \n",
    "        gold_annotations = map(lambda annotation: normalize_standoff_annotation(annotation), gold_annotations_standoff) \n",
    "        ev = Evaluator.Evaluator(gold_terms=set(gold_annotations), pred_terms=set(mer_annotations))\n",
    "        \n",
    "        assert not (ev.precision() > ev.f1_score() and ev.recall() > ev.f1_score())\n",
    "        assert not (ev.precision() > ev.f1_score() and ev.recall() > ev.f1_score())\n",
    "                                                                         \n",
    "        precisions.append(ev.precision())\n",
    "        recalls.append(ev.recall())\n",
    "        fscores.append(ev.f1_score())\n",
    "\n",
    "    macro_precision = sum(precisions) / number_documents\n",
    "    macro_recall = sum(recalls)/number_documents\n",
    "    macro_f1_score = sum(fscores)/number_documents\n",
    "\n",
    "    print \"Macro Precision: \" + str(macro_precision)\n",
    "    print \"Macro Recall: \" + str(macro_recall)\n",
    "    print \"Macro F1-Score: \" + str(macro_f1_score)\n",
    "    \n",
    "def micro_evaluation(path_to_gold_annotations, path_to_test_annotations):\n",
    "    \n",
    "    document_ids = os.listdir(path_to_test_annotations)\n",
    "    # Remove extension\n",
    "    document_ids = map(lambda document_id: document_id.split('.')[0], document_ids)\n",
    "    # For calculate Macro Evaluatin I use an hack with the Evaluator module\n",
    "\n",
    "    evaluator_master = Evaluator.Evaluator(set(), set())\n",
    "\n",
    "    for document_id in document_ids:\n",
    "        with open(path_to_test_annotations + document_id + '.tsv') as f:\n",
    "            mer_annotations_tsv = f.readlines()\n",
    "\n",
    "        with open(path_to_gold_annotations + document_id) as f:\n",
    "            gold_annotations_standoff = f.readlines()\n",
    "\n",
    "        mer_annotations = map(lambda annotation: normalize_tsv_mer_annotations(annotation), mer_annotations_tsv) \n",
    "        gold_annotations = map(lambda annotation: normalize_standoff_annotation(annotation), gold_annotations_standoff) \n",
    "        ev = Evaluator.Evaluator(gold_terms=set(gold_annotations), pred_terms=set(mer_annotations))\n",
    "\n",
    "        evaluator_master._y_pred += ev._y_pred\n",
    "        evaluator_master._y_true += ev._y_true\n",
    "\n",
    "    micro_precision = evaluator_master.precision()\n",
    "    micro_recall = evaluator_master.recall()\n",
    "    micro_f1_score = evaluator_master.f1_score()\n",
    "\n",
    "    print \"Micro Precision: \" + str(micro_precision)\n",
    "    print \"Micro Recall: \" + str(micro_recall)\n",
    "    print \"Micro F1-Score: \" + str(micro_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_gold_annotations = 'stand-off/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare MER Annotations with Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.512998266898\n",
      "Micro Recall: 0.470338983051\n",
      "Micro F1-Score: 0.490743299254\n",
      "\n",
      "Macro Precision: 0.482189953673\n",
      "Macro Recall: 0.484543353858\n",
      "Macro F1-Score: 0.449861437751\n"
     ]
    }
   ],
   "source": [
    "path_to_mer_annotations = 'mer_annotations/annotations/'\n",
    "\n",
    "micro_evaluation(path_to_gold_annotations, path_to_mer_annotations)\n",
    "print\n",
    "macro_evaluation(path_to_gold_annotations, path_to_mer_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Aggregated Results with Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.5\n",
      "Micro Recall: 0.77358490566\n",
      "Micro F1-Score: 0.607407407407\n",
      "\n",
      "Macro Precision: 0.520912420912\n",
      "Macro Recall: 0.782575757576\n",
      "Macro F1-Score: 0.592322291235\n"
     ]
    }
   ],
   "source": [
    "aggregated_results_path = 'aggregated_results_0.3/'\n",
    "micro_evaluation(path_to_gold_annotations, aggregated_results_path)\n",
    "print\n",
    "macro_evaluation(path_to_gold_annotations, aggregated_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.66\n",
      "Micro Recall: 0.622641509434\n",
      "Micro F1-Score: 0.640776699029\n",
      "\n",
      "Macro Precision: 0.7\n",
      "Macro Recall: 0.599494949495\n",
      "Macro F1-Score: 0.620607073239\n"
     ]
    }
   ],
   "source": [
    "aggregated_results_path = 'aggregated_results_0.5/'\n",
    "micro_evaluation(path_to_gold_annotations, aggregated_results_path)\n",
    "print\n",
    "macro_evaluation(path_to_gold_annotations, aggregated_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.814814814815\n",
      "Micro Recall: 0.415094339623\n",
      "Micro F1-Score: 0.55\n",
      "\n",
      "Macro Precision: 0.788888888889\n",
      "Macro Recall: 0.430303030303\n",
      "Macro F1-Score: 0.548015873016\n"
     ]
    }
   ],
   "source": [
    "aggregated_results_path = 'aggregated_results_0.7/'\n",
    "micro_evaluation(path_to_gold_annotations, aggregated_results_path)\n",
    "print\n",
    "macro_evaluation(path_to_gold_annotations, aggregated_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.8\n",
      "Micro Recall: 0.301886792453\n",
      "Micro F1-Score: 0.438356164384\n",
      "\n",
      "Macro Precision: 0.75\n",
      "Macro Recall: 0.29696969697\n",
      "Macro F1-Score: 0.411904761905\n"
     ]
    }
   ],
   "source": [
    "aggregated_results_path = 'aggregated_results_0.8/'\n",
    "micro_evaluation(path_to_gold_annotations, aggregated_results_path)\n",
    "print\n",
    "macro_evaluation(path_to_gold_annotations, aggregated_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
