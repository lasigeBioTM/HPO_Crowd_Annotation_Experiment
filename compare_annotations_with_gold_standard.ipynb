{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluator is a library I've created (https://github.com/LLCampos/evaluator)\n",
    "from Evaluator import Evaluator\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Normalized format is \"{index_begin}:{index_end}:{entity_name}\". For example:\n",
    "# \"191:200:carcinoma\"\n",
    "\n",
    "# Convert MER annotation into a normalized format\n",
    "# For example, converts \"191\\t200\\tcarcinoma\\n\" into \"191:200:carcinoma\"\n",
    "def normalize_tsv_mer_annotations(mer_annotation):\n",
    "    return mer_annotation.strip().replace('\\t', ':')\n",
    "\n",
    "# Convert Standoff annotation into a normalized format\n",
    "# For example, converts \"[40::58]\\tHP_0000006 | autosomal dominant\\n\" into \"40:58:autosomal dominant\"\n",
    "def normalize_standoff_annotation(standoff_annotation):\n",
    "    index_begin = re.findall('\\[(.*)::', standoff_annotation)[0]\n",
    "    index_end = re.findall('::(.*)]', standoff_annotation)[0]\n",
    "    entity_name = re.findall('\\| (.*)', standoff_annotation)[0]\n",
    "    \n",
    "    return '{}:{}:{}'.format(index_begin, index_end, entity_name)\n",
    "    \n",
    "\n",
    "def micro_evaluation(path_to_gold_annotations, path_to_test_annotations):\n",
    "    \n",
    "    document_ids = os.listdir(path_to_test_annotations)\n",
    "    # Remove extension\n",
    "    document_ids = map(lambda document_id: document_id.split('.')[0], document_ids)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    fscores = []\n",
    "    number_documents = len(document_ids)\n",
    "\n",
    "    for document_id in document_ids:\n",
    "        with open(path_to_mer_annotations + document_id + '.tsv') as f:\n",
    "            mer_annotations_tsv = f.readlines()\n",
    "\n",
    "        with open(path_to_gold_annotations + document_id) as f:\n",
    "            gold_annotations_standoff = f.readlines()\n",
    "\n",
    "        mer_annotations = map(lambda annotation: normalize_tsv_mer_annotations(annotation), mer_annotations_tsv) \n",
    "        gold_annotations = map(lambda annotation: normalize_standoff_annotation(annotation), gold_annotations_standoff) \n",
    "        ev = Evaluator.Evaluator(gold_terms=set(gold_annotations), pred_terms=set(mer_annotations))\n",
    "\n",
    "        precisions.append(ev.precision())\n",
    "        recalls.append(ev.recall())\n",
    "        fscores.append(ev.f1_score())\n",
    "\n",
    "    micro_precision = sum(precisions) / number_documents\n",
    "    micro_recall = sum(recalls)/number_documents\n",
    "    micro_f1_score = sum(fscores)/number_documents\n",
    "\n",
    "    print \"Micro Precision: \" + str(micro_precision)\n",
    "    print \"Micro Recall: \" + str(micro_recall)\n",
    "    print \"Micro F1-Score: \" + str(micro_f1_score)\n",
    "    \n",
    "def macro_evaluation(path_to_gold_annotations, path_to_test_annotations):\n",
    "    \n",
    "    document_ids = os.listdir(path_to_test_annotations)\n",
    "    # Remove extension\n",
    "    document_ids = map(lambda document_id: document_id.split('.')[0], document_ids)\n",
    "    # For calculate Macro Evaluatin I use an hack with the Evaluator module\n",
    "\n",
    "    evaluator_master = Evaluator.Evaluator(set(), set())\n",
    "\n",
    "    for document_id in document_ids:\n",
    "        with open(path_to_mer_annotations + document_id + '.tsv') as f:\n",
    "            mer_annotations_tsv = f.readlines()\n",
    "\n",
    "        with open(path_to_gold_annotations + document_id) as f:\n",
    "            gold_annotations_standoff = f.readlines()\n",
    "\n",
    "        mer_annotations = map(lambda annotation: normalize_tsv_mer_annotations(annotation), mer_annotations_tsv) \n",
    "        gold_annotations = map(lambda annotation: normalize_standoff_annotation(annotation), gold_annotations_standoff) \n",
    "        ev = Evaluator.Evaluator(gold_terms=set(gold_annotations), pred_terms=set(mer_annotations))\n",
    "\n",
    "        evaluator_master._y_pred += ev._y_pred\n",
    "        evaluator_master._y_true += ev._y_true\n",
    "\n",
    "    macro_precision = evaluator_master.precision()\n",
    "    macro_recall = evaluator_master.recall()\n",
    "    macro_f1_score = evaluator_master.f1_score()\n",
    "\n",
    "    print \"Macro Precision: \" + str(macro_precision)\n",
    "    print \"Macro Recall: \" + str(macro_recall)\n",
    "    print \"Macro F1-Score: \" + str(macro_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_gold_annotations = 'stand-off/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare MER Annotations with Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.481173529696\n",
      "Micro Recall: 0.482252086474\n",
      "Micro F1-Score: 0.448153217091\n",
      "\n",
      "Macro Precision: 0.511897852583\n",
      "Macro Recall: 0.467161016949\n",
      "Macro F1-Score: 0.488507338687\n"
     ]
    }
   ],
   "source": [
    "path_to_mer_annotations = 'mer_annotations/annotations/'\n",
    "\n",
    "micro_evaluation(path_to_gold_annotations, path_to_mer_annotations)\n",
    "print\n",
    "macro_evaluation(path_to_gold_annotations, path_to_mer_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Aggregated Results with Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.479091995221\n",
      "Micro Recall: 0.511544011544\n",
      "Micro F1-Score: 0.487340751492\n",
      "\n",
      "Macro Precision: 0.634615384615\n",
      "Macro Recall: 0.75\n",
      "Macro F1-Score: 0.6875\n"
     ]
    }
   ],
   "source": [
    "aggregated_results_path = 'aggregated_results_0.5/'\n",
    "micro_evaluation(path_to_gold_annotations, aggregated_results_path)\n",
    "print\n",
    "macro_evaluation(path_to_gold_annotations, aggregated_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
