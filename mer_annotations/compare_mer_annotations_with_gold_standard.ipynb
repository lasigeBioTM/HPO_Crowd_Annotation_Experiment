{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluator is a library I've created (https://github.com/LLCampos/evaluator)\n",
    "from Evaluator import Evaluator\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalized format is \"{index_begin}:{index_end}:{entity_name}\". For example:\n",
    "# \"191:200:carcinoma\"\n",
    "\n",
    "# Convert MER annotation into a normalized format\n",
    "# For example, converts \"191\\t200\\tcarcinoma\\n\" into \"191:200:carcinoma\"\n",
    "def normalize_tsv_mer_annotations(mer_annotation):\n",
    "    return mer_annotation.strip().replace('\\t', ':')\n",
    "\n",
    "# Convert Standoff annotation into a normalized format\n",
    "# For example, converts \"[40::58]\\tHP_0000006 | autosomal dominant\\n\" into \"40:58:autosomal dominant\"\n",
    "def normalize_standoff_annotation(standoff_annotation):\n",
    "    index_begin = re.findall('\\[(.*)::', standoff_annotation)[0]\n",
    "    index_end = re.findall('::(.*)]', standoff_annotation)[0]\n",
    "    entity_name = re.findall('\\| (.*)', standoff_annotation)[0]\n",
    "    \n",
    "    return '{}:{}:{}'.format(index_begin, index_end, entity_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_mer_annotations = 'annotations/'\n",
    "path_to_gold_annotations = '../stand-off/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_ids = os.listdir(path_to_gold_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.482382936761\n",
      "Micro Recall: 0.484478222459\n",
      "Micro F1-Score: 0.449974182127\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "fscores = []\n",
    "number_documents = len(document_ids)\n",
    "\n",
    "for document_id in document_ids:\n",
    "    with open(path_to_mer_annotations + document_id + '.tsv') as f:\n",
    "        mer_annotations_tsv = f.readlines()\n",
    "\n",
    "    with open(path_to_gold_annotations + document_id) as f:\n",
    "        gold_annotations_standoff = f.readlines()\n",
    "\n",
    "    mer_annotations = map(lambda annotation: normalize_tsv_mer_annotations(annotation), mer_annotations_tsv) \n",
    "    gold_annotations = map(lambda annotation: normalize_standoff_annotation(annotation), gold_annotations_standoff) \n",
    "    ev = Evaluator.Evaluator(gold_terms=set(gold_annotations), pred_terms=set(mer_annotations))\n",
    "    \n",
    "    precisions.append(ev.precision())\n",
    "    recalls.append(ev.recall())\n",
    "    fscores.append(ev.f1_score())\n",
    "    \n",
    "micro_precision = sum(precisions) / number_documents\n",
    "micro_recall = sum(recalls)/number_documents\n",
    "micro_f1_score = sum(fscores)/number_documents\n",
    "\n",
    "print \"Micro Precision: \" + str(micro_precision)\n",
    "print \"Micro Recall: \" + str(micro_recall)\n",
    "print \"Micro F1-Score: \" + str(micro_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.5141536684\n",
      "Macro Recall: 0.471398305085\n",
      "Macro F1-Score: 0.491848576955\n"
     ]
    }
   ],
   "source": [
    "# For calculate Macro Evaluatin I use an hack with the Evaluator module\n",
    "\n",
    "evaluator_master = Evaluator.Evaluator(set(), set())\n",
    "\n",
    "for document_id in document_ids:\n",
    "    with open(path_to_mer_annotations + document_id + '.tsv') as f:\n",
    "        mer_annotations_tsv = f.readlines()\n",
    "\n",
    "    with open(path_to_gold_annotations + document_id) as f:\n",
    "        gold_annotations_standoff = f.readlines()\n",
    "\n",
    "    mer_annotations = map(lambda annotation: normalize_tsv_mer_annotations(annotation), mer_annotations_tsv) \n",
    "    gold_annotations = map(lambda annotation: normalize_standoff_annotation(annotation), gold_annotations_standoff) \n",
    "    ev = Evaluator.Evaluator(gold_terms=set(gold_annotations), pred_terms=set(mer_annotations))\n",
    "    \n",
    "    evaluator_master._y_pred += ev._y_pred\n",
    "    evaluator_master._y_true += ev._y_true\n",
    "    \n",
    "macro_precision = evaluator_master.precision()\n",
    "macro_recall = evaluator_master.recall()\n",
    "macro_f1_score = evaluator_master.f1_score()\n",
    "\n",
    "print \"Macro Precision: \" + str(macro_precision)\n",
    "print \"Macro Recall: \" + str(macro_recall)\n",
    "print \"Macro F1-Score: \" + str(macro_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
